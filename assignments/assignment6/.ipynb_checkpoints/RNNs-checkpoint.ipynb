{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!conda install bokeh=0.13.0\\n!conda install gensim=3.6.0\\n!conda install nltk\\n!conda install install scikit-learn=0.20.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''!conda install bokeh=0.13.0\n",
    "!conda install gensim=3.6.0\n",
    "!conda install nltk\n",
    "!conda install install scikit-learn=0.20.2'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiA2dGmgF1rW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QstS4NO0L97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTai8Ta0lgwL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'DET', 'VERB', 'ADV', 'PRON', 'PRT', 'ADP', 'X', 'CONJ', '.', 'NOUN', 'NUM', 'ADJ'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URC1B2nvPGFt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rWmSToIaeAo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XCuxEBVbOY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4XsRII5kW5x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 4), (32, 4))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count)\n",
    "        self.linear = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedding = self.embedding(inputs)\n",
    "        lstm_res, _  = self.lstm(embedding)\n",
    "        result = self.linear(lstm_res)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 13])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0761,  0.1119, -0.1415,  ..., -0.0461,  0.1423, -0.1671],\n",
       "         [-0.0619,  0.0766, -0.0019,  ..., -0.0182, -0.0130,  0.0518],\n",
       "         [-0.0594,  0.1529, -0.0597,  ...,  0.0688,  0.1269,  0.0396],\n",
       "         [-0.1070,  0.1395,  0.0207,  ..., -0.0124,  0.1397, -0.0263]],\n",
       "\n",
       "        [[-0.1711,  0.1613, -0.0876,  ...,  0.0479,  0.1524,  0.0124],\n",
       "         [ 0.0017,  0.1007,  0.0626,  ...,  0.0082,  0.0501, -0.0384],\n",
       "         [-0.0977,  0.1999, -0.1852,  ...,  0.0451,  0.1407,  0.0038],\n",
       "         [-0.1396,  0.1632, -0.0783,  ..., -0.0399,  0.1018, -0.0739]],\n",
       "\n",
       "        [[-0.0811,  0.1626, -0.1414,  ...,  0.0266,  0.1181,  0.0250],\n",
       "         [-0.0273,  0.1604, -0.0311,  ...,  0.1476, -0.0604,  0.1079],\n",
       "         [-0.1737,  0.0642, -0.0478,  ...,  0.0846,  0.0443,  0.0037],\n",
       "         [-0.1873,  0.0755, -0.1383,  ..., -0.0722,  0.1763, -0.1019]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.1575,  0.0500, -0.2912,  ...,  0.0956,  0.2930,  0.0854],\n",
       "         [-0.1504,  0.0577, -0.2856,  ...,  0.0966,  0.2903,  0.0838],\n",
       "         [-0.0848, -0.0227, -0.1137,  ..., -0.0018, -0.0103, -0.0031],\n",
       "         [-0.1502,  0.0572, -0.2894,  ...,  0.0972,  0.2884,  0.0819]],\n",
       "\n",
       "        [[-0.1576,  0.0499, -0.2912,  ...,  0.0956,  0.2932,  0.0855],\n",
       "         [-0.1524,  0.0560, -0.2874,  ...,  0.0960,  0.2912,  0.0844],\n",
       "         [-0.1767,  0.0135, -0.0982,  ..., -0.0478,  0.0533, -0.0472],\n",
       "         [-0.1523,  0.0553, -0.2903,  ...,  0.0965,  0.2897,  0.0829]],\n",
       "\n",
       "        [[-0.1577,  0.0499, -0.2912,  ...,  0.0956,  0.2933,  0.0856],\n",
       "         [-0.1538,  0.0546, -0.2886,  ...,  0.0957,  0.2918,  0.0848],\n",
       "         [-0.0258,  0.0564, -0.0458,  ...,  0.0125,  0.0554,  0.0028],\n",
       "         [-0.1538,  0.0539, -0.2908,  ...,  0.0961,  0.2906,  0.0837]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits\n",
    "#подается batch из слов с контекстом 4 слова в каждой строке и выдаются числа связывающее каждое слово в batch с batch тегов со всеми тегами из набора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.argmax(logits, 2)\n",
    "#в каждом из 13 наборов тегов надо найти максимальное число. Максимальне число будет сооветсвовать тегу из всех 13 уникальных тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.0625\n"
     ]
    }
   ],
   "source": [
    "accuracy = float(torch.sum(preds == y_batch)) / (y_batch.shape[0] * y_batch.shape[1])\n",
    "print(f'Accuracy = {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMUyUm1hgpe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 2.5647268295288086\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "loss = criterion(logits.reshape(-1, 13), y_batch.reshape(-1))\n",
    "print(f'Loss = {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "\n",
    "                y_batch = y_batch.reshape(-1)\n",
    "                               \n",
    "                loss = criterion(logits.reshape(-1, 13), y_batch)\n",
    "\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                pred = torch.argmax(logits, 2)\n",
    "                \n",
    "                pred = pred.reshape(-1)\n",
    "                mask = torch.zeros_like(y_batch)\n",
    "                mask[y_batch != 0] = 1\n",
    "                padding = torch.sum(mask == 0)\n",
    "                pred = pred * mask\n",
    "\n",
    "                cur_correct_count, cur_sum_count = torch.sum(pred == y_batch)- padding, y_batch.shape[0] - padding\n",
    "\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), float(cur_correct_count) / float(cur_sum_count))\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, float(correct_count) / float(sum_count))\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, float(correct_count) / float(sum_count)\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.52412, Accuracy = 82.64%: 100%|█████████████████████████████| 572/572 [00:10<00:00, 55.03it/s]\n",
      "[1 / 50]   Val: Loss = 0.30909, Accuracy = 89.89%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 89.15it/s]\n",
      "[2 / 50] Train: Loss = 0.25441, Accuracy = 91.87%: 100%|█████████████████████████████| 572/572 [00:10<00:00, 55.47it/s]\n",
      "[2 / 50]   Val: Loss = 0.23937, Accuracy = 92.37%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 96.03it/s]\n",
      "[3 / 50] Train: Loss = 0.21541, Accuracy = 93.19%: 100%|█████████████████████████████| 572/572 [00:10<00:00, 55.16it/s]\n",
      "[3 / 50]   Val: Loss = 0.21087, Accuracy = 93.40%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 93.30it/s]\n",
      "[4 / 50] Train: Loss = 0.19256, Accuracy = 93.94%: 100%|█████████████████████████████| 572/572 [00:10<00:00, 53.95it/s]\n",
      "[4 / 50]   Val: Loss = 0.19669, Accuracy = 93.60%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 96.77it/s]\n",
      "[5 / 50] Train: Loss = 0.18115, Accuracy = 94.32%: 100%|█████████████████████████████| 572/572 [00:10<00:00, 54.62it/s]\n",
      "[5 / 50]   Val: Loss = 0.18948, Accuracy = 93.87%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 96.73it/s]\n",
      "[6 / 50] Train: Loss = 0.17468, Accuracy = 94.48%: 100%|█████████████████████████████| 572/572 [00:10<00:00, 53.54it/s]\n",
      "[6 / 50]   Val: Loss = 0.18291, Accuracy = 94.24%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 98.94it/s]\n",
      "[7 / 50] Train: Loss = 0.16967, Accuracy = 94.67%: 100%|█████████████████████████████| 572/572 [00:10<00:00, 53.34it/s]\n",
      "[7 / 50]   Val: Loss = 0.17966, Accuracy = 94.45%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 94.45it/s]\n",
      "[8 / 50] Train: Loss = 0.16674, Accuracy = 94.75%: 100%|█████████████████████████████| 572/572 [00:10<00:00, 53.31it/s]\n",
      "[8 / 50]   Val: Loss = 0.19225, Accuracy = 93.47%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 93.48it/s]\n",
      "[9 / 50] Train: Loss = 0.16457, Accuracy = 94.83%: 100%|█████████████████████████████| 572/572 [00:10<00:00, 54.33it/s]\n",
      "[9 / 50]   Val: Loss = 0.17759, Accuracy = 94.47%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 96.72it/s]\n",
      "[10 / 50] Train: Loss = 0.16267, Accuracy = 94.88%: 100%|████████████████████████████| 572/572 [00:10<00:00, 54.53it/s]\n",
      "[10 / 50]   Val: Loss = 0.17537, Accuracy = 94.36%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 100.53it/s]\n",
      "[11 / 50] Train: Loss = 0.16090, Accuracy = 94.95%: 100%|████████████████████████████| 572/572 [00:10<00:00, 54.01it/s]\n",
      "[11 / 50]   Val: Loss = 0.17590, Accuracy = 94.44%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 95.34it/s]\n",
      "[12 / 50] Train: Loss = 0.16016, Accuracy = 94.97%: 100%|████████████████████████████| 572/572 [00:10<00:00, 54.61it/s]\n",
      "[12 / 50]   Val: Loss = 0.17205, Accuracy = 94.49%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 93.97it/s]\n",
      "[13 / 50] Train: Loss = 0.15926, Accuracy = 94.98%: 100%|████████████████████████████| 572/572 [00:10<00:00, 54.65it/s]\n",
      "[13 / 50]   Val: Loss = 0.17061, Accuracy = 94.74%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 93.90it/s]\n",
      "[14 / 50] Train: Loss = 0.15889, Accuracy = 95.01%: 100%|████████████████████████████| 572/572 [00:10<00:00, 54.72it/s]\n",
      "[14 / 50]   Val: Loss = 0.18621, Accuracy = 94.02%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 92.33it/s]\n",
      "[15 / 50] Train: Loss = 0.15825, Accuracy = 95.05%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.06it/s]\n",
      "[15 / 50]   Val: Loss = 0.17192, Accuracy = 94.56%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 87.60it/s]\n",
      "[16 / 50] Train: Loss = 0.15832, Accuracy = 95.01%: 100%|████████████████████████████| 572/572 [00:10<00:00, 54.11it/s]\n",
      "[16 / 50]   Val: Loss = 0.16974, Accuracy = 94.71%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 91.19it/s]\n",
      "[17 / 50] Train: Loss = 0.15771, Accuracy = 95.06%: 100%|████████████████████████████| 572/572 [00:10<00:00, 54.75it/s]\n",
      "[17 / 50]   Val: Loss = 0.17200, Accuracy = 94.53%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 93.28it/s]\n",
      "[18 / 50] Train: Loss = 0.15775, Accuracy = 95.05%: 100%|████████████████████████████| 572/572 [00:10<00:00, 54.14it/s]\n",
      "[18 / 50]   Val: Loss = 0.17300, Accuracy = 94.60%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 99.69it/s]\n",
      "[19 / 50] Train: Loss = 0.15756, Accuracy = 95.08%: 100%|████████████████████████████| 572/572 [00:10<00:00, 54.59it/s]\n",
      "[19 / 50]   Val: Loss = 0.17274, Accuracy = 94.68%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 93.34it/s]\n",
      "[20 / 50] Train: Loss = 0.15701, Accuracy = 95.09%: 100%|████████████████████████████| 572/572 [00:10<00:00, 54.77it/s]\n",
      "[20 / 50]   Val: Loss = 0.17618, Accuracy = 94.37%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 96.72it/s]\n",
      "[21 / 50] Train: Loss = 0.15684, Accuracy = 95.07%: 100%|████████████████████████████| 572/572 [00:10<00:00, 54.91it/s]\n",
      "[21 / 50]   Val: Loss = 0.17139, Accuracy = 94.72%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 95.29it/s]\n",
      "[22 / 50] Train: Loss = 0.15687, Accuracy = 95.07%: 100%|████████████████████████████| 572/572 [00:10<00:00, 54.58it/s]\n",
      "[22 / 50]   Val: Loss = 0.17154, Accuracy = 94.52%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 98.91it/s]\n",
      "[23 / 50] Train: Loss = 0.15685, Accuracy = 95.08%: 100%|████████████████████████████| 572/572 [00:10<00:00, 54.72it/s]\n",
      "[23 / 50]   Val: Loss = 0.16911, Accuracy = 94.75%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 98.89it/s]\n",
      "[24 / 50] Train: Loss = 0.15675, Accuracy = 95.07%: 100%|████████████████████████████| 572/572 [00:10<00:00, 54.74it/s]\n",
      "[24 / 50]   Val: Loss = 0.17476, Accuracy = 94.38%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 97.46it/s]\n",
      "[25 / 50] Train: Loss = 0.15627, Accuracy = 95.09%: 100%|████████████████████████████| 572/572 [00:10<00:00, 54.59it/s]\n",
      "[25 / 50]   Val: Loss = 0.17114, Accuracy = 94.56%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 93.98it/s]\n",
      "[26 / 50] Train: Loss = 0.15613, Accuracy = 95.08%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.20it/s]\n",
      "[26 / 50]   Val: Loss = 0.17052, Accuracy = 94.65%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 93.97it/s]\n",
      "[27 / 50] Train: Loss = 0.15611, Accuracy = 95.10%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.19it/s]\n",
      "[27 / 50]   Val: Loss = 0.17170, Accuracy = 94.72%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 103.97it/s]\n",
      "[28 / 50] Train: Loss = 0.15633, Accuracy = 95.07%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.31it/s]\n",
      "[28 / 50]   Val: Loss = 0.17051, Accuracy = 94.72%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 100.49it/s]\n",
      "[29 / 50] Train: Loss = 0.15612, Accuracy = 95.08%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.51it/s]\n",
      "[29 / 50]   Val: Loss = 0.17123, Accuracy = 94.70%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 98.25it/s]\n",
      "[30 / 50] Train: Loss = 0.15670, Accuracy = 95.09%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.53it/s]\n",
      "[30 / 50]   Val: Loss = 0.17245, Accuracy = 94.61%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 98.96it/s]\n",
      "[31 / 50] Train: Loss = 0.15640, Accuracy = 95.07%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.54it/s]\n",
      "[31 / 50]   Val: Loss = 0.17059, Accuracy = 94.62%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 99.25it/s]\n",
      "[32 / 50] Train: Loss = 0.15629, Accuracy = 95.09%: 100%|████████████████████████████| 572/572 [00:10<00:00, 56.05it/s]\n",
      "[32 / 50]   Val: Loss = 0.17311, Accuracy = 94.75%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 93.01it/s]\n",
      "[33 / 50] Train: Loss = 0.15617, Accuracy = 95.10%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.44it/s]\n",
      "[33 / 50]   Val: Loss = 0.16983, Accuracy = 94.70%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 94.68it/s]\n",
      "[34 / 50] Train: Loss = 0.15624, Accuracy = 95.07%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.68it/s]\n",
      "[34 / 50]   Val: Loss = 0.16919, Accuracy = 94.63%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 97.47it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[35 / 50] Train: Loss = 0.15656, Accuracy = 95.08%: 100%|████████████████████████████| 572/572 [00:10<00:00, 56.05it/s]\n",
      "[35 / 50]   Val: Loss = 0.17190, Accuracy = 94.76%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 92.42it/s]\n",
      "[36 / 50] Train: Loss = 0.15598, Accuracy = 95.07%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.70it/s]\n",
      "[36 / 50]   Val: Loss = 0.17313, Accuracy = 94.68%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 93.90it/s]\n",
      "[37 / 50] Train: Loss = 0.15637, Accuracy = 95.08%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.88it/s]\n",
      "[37 / 50]   Val: Loss = 0.17344, Accuracy = 94.57%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 96.78it/s]\n",
      "[38 / 50] Train: Loss = 0.15628, Accuracy = 95.09%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.50it/s]\n",
      "[38 / 50]   Val: Loss = 0.17427, Accuracy = 94.48%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 88.60it/s]\n",
      "[39 / 50] Train: Loss = 0.15577, Accuracy = 95.10%: 100%|████████████████████████████| 572/572 [00:10<00:00, 56.09it/s]\n",
      "[39 / 50]   Val: Loss = 0.17203, Accuracy = 94.61%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 101.26it/s]\n",
      "[40 / 50] Train: Loss = 0.15556, Accuracy = 95.12%: 100%|████████████████████████████| 572/572 [00:10<00:00, 56.08it/s]\n",
      "[40 / 50]   Val: Loss = 0.17159, Accuracy = 94.65%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 95.35it/s]\n",
      "[41 / 50] Train: Loss = 0.15638, Accuracy = 95.10%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.82it/s]\n",
      "[41 / 50]   Val: Loss = 0.16917, Accuracy = 94.70%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 101.18it/s]\n",
      "[42 / 50] Train: Loss = 0.15604, Accuracy = 95.13%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.72it/s]\n",
      "[42 / 50]   Val: Loss = 0.17059, Accuracy = 94.63%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 104.12it/s]\n",
      "[43 / 50] Train: Loss = 0.15647, Accuracy = 95.09%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.59it/s]\n",
      "[43 / 50]   Val: Loss = 0.17522, Accuracy = 94.44%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 98.22it/s]\n",
      "[44 / 50] Train: Loss = 0.15605, Accuracy = 95.12%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.51it/s]\n",
      "[44 / 50]   Val: Loss = 0.17553, Accuracy = 94.68%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 101.33it/s]\n",
      "[45 / 50] Train: Loss = 0.15607, Accuracy = 95.11%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.78it/s]\n",
      "[45 / 50]   Val: Loss = 0.17189, Accuracy = 94.73%: 100%|█████████████████████████████| 13/13 [00:00<00:00, 102.86it/s]\n",
      "[46 / 50] Train: Loss = 0.15575, Accuracy = 95.11%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.56it/s]\n",
      "[46 / 50]   Val: Loss = 0.17043, Accuracy = 94.74%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 92.56it/s]\n",
      "[47 / 50] Train: Loss = 0.15626, Accuracy = 95.09%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.96it/s]\n",
      "[47 / 50]   Val: Loss = 0.17495, Accuracy = 94.64%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 95.52it/s]\n",
      "[48 / 50] Train: Loss = 0.15612, Accuracy = 95.10%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.84it/s]\n",
      "[48 / 50]   Val: Loss = 0.17022, Accuracy = 94.60%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 98.23it/s]\n",
      "[49 / 50] Train: Loss = 0.15605, Accuracy = 95.10%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.88it/s]\n",
      "[49 / 50]   Val: Loss = 0.17008, Accuracy = 94.67%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 98.21it/s]\n",
      "[50 / 50] Train: Loss = 0.15552, Accuracy = 95.13%: 100%|████████████████████████████| 572/572 [00:10<00:00, 55.03it/s]\n",
      "[50 / 50]   Val: Loss = 0.17235, Accuracy = 94.52%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 98.98it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 5e-3, weight_decay = 5e-4)\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98wr38_rw55D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 94.5908203125 %\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data, batch_size=64):\n",
    "    model.eval() # Evaluation mode\n",
    "    val_accuracy = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "        X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "        logits = model(X_batch)\n",
    "        \n",
    "        pred = torch.argmax(logits, 2)\n",
    "        \n",
    "        mask = (y_batch != 0).float()\n",
    "        \n",
    "        correct += ((pred == y_batch).float() * mask).sum()\n",
    "        \n",
    "        total += mask.sum()       \n",
    "        \n",
    "    val_accuracy = float(correct)/total\n",
    "        \n",
    "    return val_accuracy\n",
    "\n",
    "test_ac =  compute_accuracy(model, (X_test, y_test))\n",
    "print(f'Test accuracy is {test_ac * 100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, lstm_layers_count, bidirectional=True)\n",
    "        self.linear = nn.Linear(lstm_hidden_dim * 2, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedding = self.embedding(inputs)\n",
    "        lstm_res, _  = self.lstm(embedding)\n",
    "        result = self.linear(lstm_res)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 20] Train: Loss = 0.55493, Accuracy = 82.50%: 100%|█████████████████████████████| 572/572 [00:18<00:00, 30.80it/s]\n",
      "[1 / 20]   Val: Loss = 0.27868, Accuracy = 91.03%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 63.70it/s]\n",
      "[2 / 20] Train: Loss = 0.20628, Accuracy = 93.45%: 100%|█████████████████████████████| 572/572 [00:18<00:00, 30.85it/s]\n",
      "[2 / 20]   Val: Loss = 0.18319, Accuracy = 94.37%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 61.03it/s]\n",
      "[3 / 20] Train: Loss = 0.13080, Accuracy = 95.96%: 100%|█████████████████████████████| 572/572 [00:18<00:00, 31.08it/s]\n",
      "[3 / 20]   Val: Loss = 0.14742, Accuracy = 95.56%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 63.78it/s]\n",
      "[4 / 20] Train: Loss = 0.08918, Accuracy = 97.29%: 100%|█████████████████████████████| 572/572 [00:18<00:00, 30.82it/s]\n",
      "[4 / 20]   Val: Loss = 0.13257, Accuracy = 96.14%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 63.36it/s]\n",
      "[5 / 20] Train: Loss = 0.06145, Accuracy = 98.16%: 100%|█████████████████████████████| 572/572 [00:18<00:00, 31.02it/s]\n",
      "[5 / 20]   Val: Loss = 0.12268, Accuracy = 96.44%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 63.07it/s]\n",
      "[6 / 20] Train: Loss = 0.04183, Accuracy = 98.78%: 100%|█████████████████████████████| 572/572 [00:18<00:00, 31.07it/s]\n",
      "[6 / 20]   Val: Loss = 0.12731, Accuracy = 96.57%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 65.95it/s]\n",
      "[7 / 20] Train: Loss = 0.02808, Accuracy = 99.23%: 100%|█████████████████████████████| 572/572 [00:18<00:00, 30.86it/s]\n",
      "[7 / 20]   Val: Loss = 0.12604, Accuracy = 96.55%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 62.77it/s]\n",
      "[8 / 20] Train: Loss = 0.01827, Accuracy = 99.53%: 100%|█████████████████████████████| 572/572 [00:18<00:00, 31.11it/s]\n",
      "[8 / 20]   Val: Loss = 0.13213, Accuracy = 96.66%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 59.64it/s]\n",
      "[9 / 20] Train: Loss = 0.01180, Accuracy = 99.73%: 100%|█████████████████████████████| 572/572 [00:18<00:00, 30.87it/s]\n",
      "[9 / 20]   Val: Loss = 0.13574, Accuracy = 96.70%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 61.91it/s]\n",
      "[10 / 20] Train: Loss = 0.00726, Accuracy = 99.85%: 100%|████████████████████████████| 572/572 [00:18<00:00, 30.94it/s]\n",
      "[10 / 20]   Val: Loss = 0.14612, Accuracy = 96.71%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 62.35it/s]\n",
      "[11 / 20] Train: Loss = 0.00439, Accuracy = 99.93%: 100%|████████████████████████████| 572/572 [00:18<00:00, 30.16it/s]\n",
      "[11 / 20]   Val: Loss = 0.15613, Accuracy = 96.69%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 61.28it/s]\n",
      "[12 / 20] Train: Loss = 0.00281, Accuracy = 99.96%: 100%|████████████████████████████| 572/572 [00:18<00:00, 30.46it/s]\n",
      "[12 / 20]   Val: Loss = 0.16201, Accuracy = 96.66%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 67.30it/s]\n",
      "[13 / 20] Train: Loss = 0.00199, Accuracy = 99.98%: 100%|████████████████████████████| 572/572 [00:18<00:00, 31.07it/s]\n",
      "[13 / 20]   Val: Loss = 0.17260, Accuracy = 96.68%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 60.05it/s]\n",
      "[14 / 20] Train: Loss = 0.00141, Accuracy = 99.99%: 100%|████████████████████████████| 572/572 [00:18<00:00, 30.97it/s]\n",
      "[14 / 20]   Val: Loss = 0.17110, Accuracy = 96.75%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 62.92it/s]\n",
      "[15 / 20] Train: Loss = 0.00110, Accuracy = 99.98%: 100%|████████████████████████████| 572/572 [00:18<00:00, 30.87it/s]\n",
      "[15 / 20]   Val: Loss = 0.18584, Accuracy = 96.66%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 61.74it/s]\n",
      "[16 / 20] Train: Loss = 0.00336, Accuracy = 99.91%: 100%|████████████████████████████| 572/572 [00:18<00:00, 30.85it/s]\n",
      "[16 / 20]   Val: Loss = 0.19264, Accuracy = 96.63%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 61.31it/s]\n",
      "[17 / 20] Train: Loss = 0.00193, Accuracy = 99.96%: 100%|████████████████████████████| 572/572 [00:18<00:00, 30.83it/s]\n",
      "[17 / 20]   Val: Loss = 0.19575, Accuracy = 96.68%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 63.08it/s]\n",
      "[18 / 20] Train: Loss = 0.00068, Accuracy = 99.99%: 100%|████████████████████████████| 572/572 [00:18<00:00, 30.76it/s]\n",
      "[18 / 20]   Val: Loss = 0.19837, Accuracy = 96.80%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 61.40it/s]\n",
      "[19 / 20] Train: Loss = 0.00031, Accuracy = 100.00%: 100%|███████████████████████████| 572/572 [00:18<00:00, 30.95it/s]\n",
      "[19 / 20]   Val: Loss = 0.20199, Accuracy = 96.81%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 59.63it/s]\n",
      "[20 / 20] Train: Loss = 0.00027, Accuracy = 100.00%: 100%|███████████████████████████| 572/572 [00:18<00:00, 30.95it/s]\n",
      "[20 / 20]   Val: Loss = 0.21515, Accuracy = 96.75%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 61.57it/s]\n"
     ]
    }
   ],
   "source": [
    "bidirectional_model = BidirectionalLSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0).cuda()\n",
    "optimizer = optim.Adam(bidirectional_model.parameters())\n",
    "\n",
    "fit(bidirectional_model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for biderectional LSTM is 96.75904083251953 %\n"
     ]
    }
   ],
   "source": [
    "test_ac =  compute_accuracy(bidirectional_model, (X_test, y_test))\n",
    "print(f'Test accuracy for biderectional LSTM is {test_ac * 100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsCstxiO03oT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat"
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings)\n",
    "        self.lstm = nn.LSTM(self.embedding.embedding_dim, lstm_hidden_dim, lstm_layers_count, bidirectional=True)\n",
    "        self.linear = nn.Linear(lstm_hidden_dim * 2, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedding = self.embedding(inputs)\n",
    "        lstm_res, _  = self.lstm(embedding)\n",
    "        result = self.linear(lstm_res)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 20] Train: Loss = 0.53565, Accuracy = 83.61%: 100%|█████████████████████████████| 572/572 [00:31<00:00, 18.08it/s]\n",
      "[1 / 20]   Val: Loss = 0.19269, Accuracy = 94.24%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 46.92it/s]\n",
      "[2 / 20] Train: Loss = 0.13309, Accuracy = 95.98%: 100%|█████████████████████████████| 572/572 [00:31<00:00, 18.04it/s]\n",
      "[2 / 20]   Val: Loss = 0.12970, Accuracy = 95.95%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 48.29it/s]\n",
      "[3 / 20] Train: Loss = 0.09551, Accuracy = 97.04%: 100%|█████████████████████████████| 572/572 [00:31<00:00, 18.03it/s]\n",
      "[3 / 20]   Val: Loss = 0.10911, Accuracy = 96.54%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 46.72it/s]\n",
      "[4 / 20] Train: Loss = 0.07946, Accuracy = 97.50%: 100%|█████████████████████████████| 572/572 [00:31<00:00, 18.17it/s]\n",
      "[4 / 20]   Val: Loss = 0.10039, Accuracy = 96.81%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 48.91it/s]\n",
      "[5 / 20] Train: Loss = 0.06946, Accuracy = 97.79%: 100%|█████████████████████████████| 572/572 [00:31<00:00, 18.14it/s]\n",
      "[5 / 20]   Val: Loss = 0.09637, Accuracy = 96.92%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 47.88it/s]\n",
      "[6 / 20] Train: Loss = 0.06273, Accuracy = 98.00%: 100%|█████████████████████████████| 572/572 [00:31<00:00, 18.14it/s]\n",
      "[6 / 20]   Val: Loss = 0.09328, Accuracy = 97.04%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 47.21it/s]\n",
      "[7 / 20] Train: Loss = 0.05699, Accuracy = 98.15%: 100%|█████████████████████████████| 572/572 [00:31<00:00, 18.34it/s]\n",
      "[7 / 20]   Val: Loss = 0.09093, Accuracy = 97.11%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 46.47it/s]\n",
      "[8 / 20] Train: Loss = 0.05276, Accuracy = 98.29%: 100%|█████████████████████████████| 572/572 [00:32<00:00, 17.80it/s]\n",
      "[8 / 20]   Val: Loss = 0.08846, Accuracy = 97.15%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 47.13it/s]\n",
      "[9 / 20] Train: Loss = 0.04906, Accuracy = 98.40%: 100%|█████████████████████████████| 572/572 [00:31<00:00, 17.89it/s]\n",
      "[9 / 20]   Val: Loss = 0.09022, Accuracy = 97.19%: 100%|███████████████████████████████| 13/13 [00:00<00:00, 44.48it/s]\n",
      "[10 / 20] Train: Loss = 0.04524, Accuracy = 98.52%: 100%|████████████████████████████| 572/572 [00:32<00:00, 17.85it/s]\n",
      "[10 / 20]   Val: Loss = 0.08861, Accuracy = 97.21%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 48.69it/s]\n",
      "[11 / 20] Train: Loss = 0.04237, Accuracy = 98.60%: 100%|████████████████████████████| 572/572 [00:31<00:00, 18.16it/s]\n",
      "[11 / 20]   Val: Loss = 0.09009, Accuracy = 97.24%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.08it/s]\n",
      "[12 / 20] Train: Loss = 0.03935, Accuracy = 98.71%: 100%|████████████████████████████| 572/572 [00:31<00:00, 18.10it/s]\n",
      "[12 / 20]   Val: Loss = 0.09020, Accuracy = 97.29%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 45.39it/s]\n",
      "[13 / 20] Train: Loss = 0.03651, Accuracy = 98.81%: 100%|████████████████████████████| 572/572 [00:31<00:00, 18.16it/s]\n",
      "[13 / 20]   Val: Loss = 0.09119, Accuracy = 97.25%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 44.75it/s]\n",
      "[14 / 20] Train: Loss = 0.03396, Accuracy = 98.89%: 100%|████████████████████████████| 572/572 [00:31<00:00, 18.13it/s]\n",
      "[14 / 20]   Val: Loss = 0.09345, Accuracy = 97.22%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.36it/s]\n",
      "[15 / 20] Train: Loss = 0.03176, Accuracy = 98.97%: 100%|████████████████████████████| 572/572 [00:31<00:00, 18.11it/s]\n",
      "[15 / 20]   Val: Loss = 0.09689, Accuracy = 97.21%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 45.96it/s]\n",
      "[16 / 20] Train: Loss = 0.02936, Accuracy = 99.04%: 100%|████████████████████████████| 572/572 [00:31<00:00, 18.15it/s]\n",
      "[16 / 20]   Val: Loss = 0.09506, Accuracy = 97.29%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.64it/s]\n",
      "[17 / 20] Train: Loss = 0.02706, Accuracy = 99.12%: 100%|████████████████████████████| 572/572 [00:31<00:00, 18.03it/s]\n",
      "[17 / 20]   Val: Loss = 0.10198, Accuracy = 97.15%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 43.46it/s]\n",
      "[18 / 20] Train: Loss = 0.02497, Accuracy = 99.18%: 100%|████████████████████████████| 572/572 [00:31<00:00, 18.02it/s]\n",
      "[18 / 20]   Val: Loss = 0.10896, Accuracy = 97.10%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 48.46it/s]\n",
      "[19 / 20] Train: Loss = 0.02297, Accuracy = 99.26%: 100%|████████████████████████████| 572/572 [00:31<00:00, 18.14it/s]\n",
      "[19 / 20]   Val: Loss = 0.10518, Accuracy = 97.12%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 49.45it/s]\n",
      "[20 / 20] Train: Loss = 0.02122, Accuracy = 99.32%: 100%|████████████████████████████| 572/572 [00:31<00:00, 18.23it/s]\n",
      "[20 / 20]   Val: Loss = 0.10955, Accuracy = 97.15%: 100%|██████████████████████████████| 13/13 [00:00<00:00, 45.80it/s]\n"
     ]
    }
   ],
   "source": [
    "model_pretrained = LSTMTaggerWithPretrainedEmbs(\n",
    "    lstm_layers_count=2,\n",
    "    embeddings= torch.FloatTensor(embeddings),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model_pretrained.parameters())\n",
    "\n",
    "fit(model_pretrained, criterion, optimizer, train_data=(X_train, y_train), epochs_count=20,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPUuAPGhEGVR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for biderectional LSTM with pretrained embeddings is 97.14014434814453 %\n"
     ]
    }
   ],
   "source": [
    "test_ac =  compute_accuracy(model_pretrained, (X_test, y_test))\n",
    "print(f'Test accuracy for biderectional LSTM with pretrained embeddings is {test_ac * 100} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 06 - RNNs, part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
